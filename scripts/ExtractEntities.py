import json
import sys
from pathlib import Path

import spacy
from flair.data import Sentence
from flair.nn import Classifier
from loguru import logger

sys.path.append(str(Path(__file__).parent.parent))

from src.memory import empty_all_memory

# def generate_entities( rows: list[str]):
# rows should be one column from dataset_rows
# https://spacy.io/usage/processing-pipelines#processing

# docs_list = list(nlp.pipe(rows))


# ents = [
#     [
#         {
#             "ent": ent.text,
#             "start": ent.start_char,
#             "end": ent.end_char,
#             "type": ent.label_,
#             "lemma": ent.lemma_,
#         }
#         for ent in doc.ents
#     ]
#     for doc in docs_list
# ]

# print(ents)

# ents_str = [json.dumps(ent, indent=4, separators=(",", ":")) for ent in ents]
# return ents_str


def please_work(val: str):
    sentence = Sentence(val, language_code="en")
    tagger = Classifier.load("ner-large")

    tagger.predict(sentence)

    print(sentence.get_labels())

    # for l in sentence:
    #     print(l)
    # print(sentence)


# def entities_from_batch(nlp:spacy.language.Language, dataset_rows):
#     source = dataset_rows["Abstract"]
#     target = dataset_rows["Highlight"]
#     hypothesis = dataset_rows["GeneratedHighlight"]


#     source_ents_str = generate_entities(nlp,source)
#     target_ents_str = generate_entities(nlp,target)
#     hypothesis_ents_str = generate_entities(nlp,hypothesis)

#     return {
#         "AbstractEntities": source_ents_str,
#         "HighlightEntities": target_ents_str,
#         "GeneratedHighlightEntities": hypothesis_ents_str,
#     }


# def generate_entities_dataset():
#     spacy.prefer_gpu()
#     nlp = spacy.load("en_core_web_trf")
#     nlp = en_core_web_trf.load()

#     logger.info("finetuned entities dataset generation started")

#     try:

#         def process_batched_rows(rows, idxs):
#             print_every = 100

#             if idxs[0] % print_every == 0 or (
#                 ((idxs[-1] // print_every) - (idxs[0] // print_every)) >= 1
#             ):
#                 print(f"Row {idxs[0]} to Row {idxs[-1]} starting...")

#             return generate_batched_entities(nlp, rows)

#         entites_ds = dataset.map(
#             function=process_batched_rows,
#             with_indices=True,
#             batched=True,
#             batch_size=1024,
#         )

#         del process_batched_rows

#         logger.success("finetuned entities dataset generation finished")
#         logger.info("started pushing finetuned entitites dataet to huggingface")
#         entites_ds.push_to_hub(entities_ds_hf_name)
#         logger.success("finetuned entitites dataset saved to huggingface as hf dataset")

#         del entites_ds
#     except Exception as e:
#         logger.exception(str(e))
#     finally:
#         del dataset
#         del nlp


def main():
    empty_all_memory()
    # nlp = spacy.blank("en")
    # llm_ner = nlp.add_pipe("llm_ner")
    # llm_ner.add_label("PERSON")
    # llm_ner.add_label("LOCATION")
    # nlp.initialize()
    # doc = nlp("Jack and Jill rode up the hill in Les Deux Alpes")
    # print([(ent.text, ent.label_) for ent in doc.ents])
    # empty_all_memory()

    # return

    # spacy.prefer_gpu()
    # nlp = assemble("./scripts/config.cfg")
    # nlp = spacy.load("en_core_web_md")
    # ents = generate_entities(
    #     # nlp,
    #     [
    #         "Red LED lights significantly reduced probability of RLR at signalized intersections. Red LED lights could reduce cognitive load for judgement about stop go decisions. Flashing green increases risk of rear end collisions due to inconsistent stopping. Countdown VMS motivated drivers positioned in the stopping zone to cross red light. Red LED is recommended as an innovative and effective treatment for RLR prevention.",
    #         "With the advent of digital publishing and online databases, the volume of textual data generated by scientific research has increased exponentially. This makes it increasingly difficult for academics to keep up with new breakthroughs and synthesise important information for their own work. Abstracts have long been a standard feature of scientific papers, providing a concise summary of the paper's content and main findings. In recent years, some journals have begun to provide research highlights as an additional summary of the paper. The aim of this article is to create research highlights automatically by using various sections of a research paper as input. We employ a pointer-generator network with a coverage mechanism and pretrained ELMo contextual embeddings to generate the highlights. Our experiments shows that the proposed model outperforms several competitive models in the literature in terms of ROUGE, METEOR, BERTScore, and MoverScore metrics.",
    #     ],
    # )

    please_work(
        "Novel catalytic route to synthesize 1Z,5Z-diene macrodiolides, inspired by Butenolide biosynthesis, achieves 65-92% yields and exceptional >97% stereoselectivity using hafnium phosphate. Unsaturated dicarboxylic acids were prepared by homo-cyclomagnesiation with SmI2, followed by Oppenauer oxidation, rather than Jones oxidation. Macrodiolides shows remarkable anti-cancer activity, inducing apoptosis in Jurkat cells through mitochondrial pathways and caspase activation, leading to cell death. These compounds inhibit phosphorylation of Akt, p38 kinases, mTOR and NF-kB pathways in cancer cells, confirmed by western blotting."
    )

    empty_all_memory()


try:
    main()
except Exception as e:
    logger.exception(e)
