import json
from typing import Literal

import torch
from datasets import DatasetDict, load_dataset
from dotenv import dotenv_values
from loguru import logger

from unsloth import FastLanguageModel  # isort:skip
from transformers import AutoModelForCausalLM, AutoTokenizer  # isort:skip


def get_mixsub():
    ds: DatasetDict = load_dataset("TRnlp/MixSub")
    # Changing all the column names to have uniform singular forms
    # All column names are now in singular form
    ds = ds.rename_column("Highlights", "Highlight")
    return ds


def generate_hallucinated_highlights(
    row,
    *args,
    instruction: str,
    model: AutoModelForCausalLM,
    tokenizer: AutoTokenizer,
    device: torch.device,
    **kwargs,
) -> dict[Literal["Hallucination"], str]:
    init_identifier = "<|start_header_id|>assistant<|end_header_id|>"
    term_identifier = "<|eot_id|>"

    abstract = row["Abstract"]
    highlight = row["Highlight"]

    ins = instruction

    mes = [
        {
            "role": "user",
            "content": f"""
            Generate hallucinated summary for the following document. You can use any method you have learned previously. #Hallucinated Summary# must not be longer than #Right Summary#.
            
            
            #Document#: {abstract}

            #Right Summary#: {highlight}
            
            #Hallucinated Summary#: 
            
            """,
        }
    ]

    row_json = ins + mes

    print(row_json)

    model_prompt = tokenizer.apply_chat_template(
        row_json,
        tokenize=False,
        add_generation_prompt=True,
    )

    model_inputs = tokenizer(
        model_prompt,
        return_tensors="pt",
        padding=True,
        truncation=True,
    ).to(device)

    model_outputs = model.generate(
        **model_inputs,
        max_new_tokens=150,
        num_return_sequences=1,
    )

    hallucination_list = tokenizer.batch_decode(
        model_outputs,
        skip_special_tokens=False,
    )

    print(hallucination_list)

    # the generaed output contains the prompt, init identifier, generated highlight and term identifier
    # the following code splits the output with init identifier, takes the second section which contains
    # the generated highlight followed by term identifier, now splits the second section based on term
    # identifier, takes the first section, which contains only the generated output. Then it strips the
    # generated content to get rid of any white spaces from the beginning and the end, and replaces
    # newline character with whitespace.
    hallucination = hallucination_list[0]
    hallucination = (
        hallucination.split(init_identifier)[1]
        .split(term_identifier)[0]
        .strip()
        .replace("\n", " ")
    )

    return {
        "Hallucination": hallucination,
    }


if __name__ == "__main__":
    config = dotenv_values(".env")
    device = "cuda" if torch.cuda.is_available() else "cpu"

    with open("./scripts/instruction/multiturn.json", "r", encoding="utf-8") as f:
        instruction = json.load(f)

    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name="unsloth/llama-2-7b-chat",
        max_seq_length=2048,
    )
    FastLanguageModel.for_inference(model)

    # mixsub = get_mixsub()

    # Map the existing dataset rows into hallucinated highlights, the returned
    # dictionary from the function passed to map, will automatically be appended
    # to the dataset on which the map function is being called, and a new dataset
    # will be returned, so note that mapping does not modify the dataset on which
    # it is being called on.
    # logger.info("hallucinated dataset generation started")
    # hal_ds = mixsub.map(
    #     generate_hallucinated_highlights,
    #     fn_kwargs={
    #         "model": model,
    #         "tokenizer": tokenizer,
    #         "device": torch.device(device),
    #     },
    # )
    # logger.info("hallucinated dataset generation finished")

    # logger.info("hallucinated dataset saving started")
    # hal_ds.push_to_hub("AdityaMayukhSom/MixSubV2-HaluEval", token=config["HF_TOKEN"])
    # logger.info("hallucinated dataset saved to huggingface as hf dataset")

    hal = generate_hallucinated_highlights(
        {
            "Abstract": "Decreasing in fossil fuel reserves and its adverse environmental effects motivate the development of the biofuels production, such as biodiesel. Animal fats residues, generated by agro-processing industries in large quantities, can be considered a low-cost raw material for conversion to alkyl esters (biodiesel). This study aims to evaluate the production of biodiesel (fatty acid methyl esters – FAME) from enzymatic catalysis applying the novel commercial lipase in liquid formulation NS-40116 obtained from the modified Thermomyces lanuginosus microorganism, using residual chicken oil as feedstock. The reaction parameters “oil to methanol molar ratio”, “water content” and “temperature” were investigated through a statistical design. At 200 rpm during 36 h, enzymatic catalysis (0.3 wt% of lipase to residual chicken oil) presented the most appropriate condition at 35 °C, 2 wt% of water content and oil to methanol molar ratio of 1:4, yielding 93.16% in FAME. Therefore, the recovery of that waste was satisfactorily performed in feasible conditions, producing a less environment-harmful biofuel.",
            "Highlight": "Low-cost residual chicken fat was proposed for enzyme-catalyzed biodiesel production.  The lipase NS-40116 from T. lanuginosus in liquid formulation was utilized.  Temperature, fat to methanol molar ratio and water content were assessed.  It was obtained 93.16% of fatty acid methyl esters (FAME) at feasible conditions.",
        },
        instruction=instruction,
        model=model,
        tokenizer=tokenizer,
        device=device,
    )

    print(hal)
