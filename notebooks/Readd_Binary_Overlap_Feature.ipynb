{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V6H5xDszloB"
      },
      "source": [
        "CELL 1: SETUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cI4xzIfQogFU",
        "outputId": "97e36f2a-3f14-47ee-a78c-db12bd4b1d4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-md==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Using GPU: Tesla T4\n",
            "Clearing initial GPU cache...\n",
            "\n",
            "Setup complete. You can now run CELL 2.\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy transformers sentencepiece requests pandas scikit-learn torch accelerate -q\n",
        "!python -m spacy download en_core_web_md\n",
        "\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Ensure GPU is available\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"WARNING: GPU not available. BART-Large requires a GPU.\")\n",
        "    device = torch.device(\"cpu\")\n",
        "else:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(\"Clearing initial GPU cache...\")\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Disable tokenizers parallelism warning\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "print(\"\\nSetup complete. You can now run CELL 2.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBNpWV1FzxZA"
      },
      "source": [
        "CELL 2: GENERATE DATA FOR MANUAL LABELING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596,
          "referenced_widgets": [
            "f0192cee25b24b91b33a13082bbcb042",
            "7b9fa13ed29440d09db352c63c3f126e",
            "62e452dfa4364736b6cf901b024f4850",
            "a2485beeb6064a5d927d6edcb803b8fe",
            "2fdded26e124457c82e7154c28f065ca",
            "f6f0a3ca3f3348ef953af6fd4c338099",
            "e63b23ff0b2641a69061e409de39e1d4",
            "4487f56314354bc4a98cce3430e2c252",
            "28325e6046dd4b80939649b0557542ec",
            "68e84dba135f4b61bcf80208ccc21a78",
            "33663a7b75414e0a8e3f59e906a46ac7"
          ]
        },
        "id": "mw4Beqmdu_lK",
        "outputId": "c5c25599-c3ca-4c33-e07c-3e71e6f2127b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- CELL 2: GENERATE DATA FOR MANUAL LABELING ---\n",
            "Loading spaCy model...\n",
            "spaCy model loaded.\n",
            "\n",
            "Loading BART-Large model (used for both prior & posterior)...\n",
            "BART model (facebook/bart-large) loaded to cuda.\n",
            "\n",
            "Clearing GPU cache before Cell 2 execution...\n",
            "Loading 500 rows from 'input.csv' to generate labeling data...\n",
            "Loaded 500 rows.\n",
            "\n",
            "Extracting entities & calculating features for 500 rows...\n",
            "--- This may take time with BART-Large ---\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0192cee25b24b91b33a13082bbcb042",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Rows for Labeling:   0%|          | 0/500 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Extracted 1619 entities for labeling.\n",
            "\n",
            "Saved detailed entity data for manual labeling to 'entities_to_label.csv'.\n",
            "\n",
            ">>> ACTION REQUIRED: <<<\n",
            "1. Download 'entities_to_label.csv'.\n",
            "2. Open it in a spreadsheet program.\n",
            "3. Add a new column named 'ManualLabel'.\n",
            "4. For relevant entity rows, fill ManualLabel with \"non hallucinated\", \"factual hallucination\", or \"non-factual hallucination\".\n",
            "   (The 'binary_overlap' column is for reference/analysis and used by the model).\n",
            "5. Save the modified file as 'manual_labels.csv'.\n",
            "6. Upload 'manual_labels.csv' to your Colab environment.\n",
            "7. Proceed to run CELL 3.\n",
            "\n",
            "Clearing GPU cache after Cell 2 execution...\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- CELL 2: GENERATE DATA FOR MANUAL LABELING ---\")\n",
        "\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import numpy as np\n",
        "import math\n",
        "from tqdm.notebook import tqdm\n",
        "import gc\n",
        "\n",
        "# --- Device Check ---\n",
        "if \"device\" not in locals():\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "    print(f\"Device re-initialized to: {device}\")\n",
        "\n",
        "print(\"Loading spaCy model...\")\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_md\")\n",
        "    print(\"spaCy model loaded.\")\n",
        "except OSError:\n",
        "    print(\"spaCy model not found. Run setup cell.\")\n",
        "    exit()\n",
        "\n",
        "print(\"\\nLoading BART-Large model (used for both prior & posterior)...\")\n",
        "model_name = \"facebook/bart-large\"\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
        "    model.eval()\n",
        "    print(f\"BART model ({model_name}) loaded to {device}.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR loading BART model: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- Config ---\n",
        "MIN_LOG_PROB_FOR_EXP = -700  # Clamp log_probs before exponentiating\n",
        "\n",
        "\n",
        "# Helper Function: Seq2Seq Log Prob (BART)\n",
        "def get_seq2seq_log_prob(model, tokenizer, input_text, target_text):\n",
        "    \"\"\"Calculates log P(target | input) using BART. Returns sum log prob.\"\"\"\n",
        "    _input_encoding = tokenizer(\n",
        "        input_text, return_tensors=\"pt\", truncation=True, max_length=1024\n",
        "    ).to(device)\n",
        "    _input_ids = _input_encoding.input_ids\n",
        "    _attention_mask = _input_encoding.attention_mask\n",
        "    _target_encoding = tokenizer(\n",
        "        target_text, return_tensors=\"pt\", truncation=True, max_length=1024\n",
        "    ).to(device)\n",
        "    _labels = _target_encoding.input_ids\n",
        "\n",
        "    if (\n",
        "        _input_ids.shape[1] == 0\n",
        "        or _labels.shape[1] == 0\n",
        "        or torch.all(\n",
        "            (_labels == tokenizer.eos_token_id) | (_labels == tokenizer.pad_token_id)\n",
        "        )\n",
        "    ):\n",
        "        return -float(\"inf\")\n",
        "\n",
        "    _total_log_prob = -float(\"inf\")\n",
        "    _outputs, _logits, _log_probs, _target_log_probs = None, None, None, None\n",
        "\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            _outputs = model(\n",
        "                input_ids=_input_ids, attention_mask=_attention_mask, labels=_labels\n",
        "            )\n",
        "            _logits = _outputs.logits\n",
        "        if _logits.shape[1] != _labels.shape[1]:\n",
        "            return -float(\"inf\")\n",
        "\n",
        "        _log_probs = F.log_softmax(_logits, dim=-1)\n",
        "        _target_log_probs = torch.gather(_log_probs, 2, _labels.unsqueeze(-1)).squeeze(\n",
        "            -1\n",
        "        )\n",
        "        _valid_token_mask = _labels != tokenizer.pad_token_id\n",
        "        if _valid_token_mask.sum() > 0:\n",
        "            _total_log_prob = _target_log_probs[_valid_token_mask].sum().item()\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        if \"out of memory\" in str(e):\n",
        "            print(\n",
        "                f\"\\nOOM Error (Seq2Seq)! InputLen:{_input_ids.shape[1]}, TargetLen:{_labels.shape[1]}. Skipping.\"\n",
        "            )\n",
        "        else:\n",
        "            print(f\"Runtime error in get_seq2seq_log_prob: {e}\")\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        return -float(\"inf\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error in get_seq2seq_log_prob: {e}\")\n",
        "        return -float(\"inf\")\n",
        "    finally:\n",
        "        if \"_outputs\" in locals() and _outputs is not None:\n",
        "            del _outputs\n",
        "        if \"_logits\" in locals() and _logits is not None:\n",
        "            del _logits\n",
        "        if \"_log_probs\" in locals() and _log_probs is not None:\n",
        "            del _log_probs\n",
        "        if \"_target_log_probs\" in locals() and _target_log_probs is not None:\n",
        "            del _target_log_probs\n",
        "        if \"_input_encoding\" in locals() and _input_encoding is not None:\n",
        "            del _input_encoding, _input_ids, _attention_mask\n",
        "        if \"_target_encoding\" in locals() and _target_encoding is not None:\n",
        "            del _target_encoding, _labels\n",
        "    return _total_log_prob if not np.isnan(_total_log_prob) else -float(\"inf\")\n",
        "\n",
        "\n",
        "# Prior and Posterior Functions - RETURNING LINEAR PROB (using BART)\n",
        "def get_prior_prob_bart(\n",
        "    entity_text: str, entity_start: int, generated_text: str\n",
        ") -> float:\n",
        "    context_ck = generated_text[:entity_start].strip()\n",
        "    if not context_ck:\n",
        "        context_ck = tokenizer.bos_token if tokenizer.bos_token else \"<|endoftext|>\"\n",
        "    # Input text = context_ck, Target text = entity_text\n",
        "    sum_log_prob = get_seq2seq_log_prob(\n",
        "        model, tokenizer, input_text=context_ck, target_text=entity_text\n",
        "    )\n",
        "\n",
        "    if sum_log_prob <= MIN_LOG_PROB_FOR_EXP:\n",
        "        return 0.0\n",
        "    try:\n",
        "        prob = math.exp(sum_log_prob)\n",
        "        return max(0.0, min(1.0, prob))\n",
        "    except (OverflowError, ValueError):\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "def get_posterior_prob_bart(\n",
        "    entity_text: str,\n",
        "    entity_start: int,\n",
        "    entity_end: int,\n",
        "    generated_text: str,\n",
        "    source_text: str,\n",
        ") -> float:\n",
        "    context_ck_before = generated_text[:entity_start].strip()\n",
        "    context_ck_after = generated_text[entity_end:].strip()\n",
        "    input_text = f\"source: {source_text.strip()} context: {context_ck_before} [SEP] {context_ck_after}\"\n",
        "    # Input text = formatted S + ck, Target text = entity_text\n",
        "    sum_log_prob = get_seq2seq_log_prob(\n",
        "        model, tokenizer, input_text=input_text, target_text=entity_text\n",
        "    )\n",
        "    if sum_log_prob <= MIN_LOG_PROB_FOR_EXP:\n",
        "        return 0.0\n",
        "    try:\n",
        "        prob = math.exp(sum_log_prob)\n",
        "        return max(0.0, min(1.0, prob))\n",
        "    except (OverflowError, ValueError):\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "# Entity Extraction Function - FOR LABELING\n",
        "def extract_entities_for_labeling(df):\n",
        "    entity_rows = []\n",
        "    print(f\"\\nExtracting entities & calculating features for {len(df)} rows...\")\n",
        "    print(f\"--- This may take time with BART-Large ---\")\n",
        "\n",
        "    row_iterator = tqdm(\n",
        "        df.iterrows(), total=len(df), desc=\"Processing Rows for Labeling\"\n",
        "    )\n",
        "    for i, row in row_iterator:\n",
        "        src_txt_original = (\n",
        "            str(row[\"ConditionedText\"]) if pd.notna(row[\"ConditionedText\"]) else \"\"\n",
        "        )\n",
        "        gen_txt = str(row[\"GeneratedText\"]) if pd.notna(row[\"GeneratedText\"]) else \"\"\n",
        "        orig_lbl = row[\"IsHallucinated\"]\n",
        "        entry_num = row.get(\"EntryNumber\", i + 1)\n",
        "\n",
        "        try:\n",
        "            max_spacy_len = nlp.max_length\n",
        "            doc = (\n",
        "                nlp(gen_txt[: max_spacy_len - 1], disable=[\"parser\"])\n",
        "                if len(gen_txt) >= max_spacy_len\n",
        "                else nlp(gen_txt, disable=[\"parser\"])\n",
        "            )\n",
        "            num_ents = len(doc.ents)\n",
        "            row_iterator.set_postfix({\"EntitiesFound\": num_ents})\n",
        "\n",
        "            if not doc.ents:\n",
        "                continue\n",
        "\n",
        "            for ent in doc.ents:\n",
        "                ent_txt = ent.text\n",
        "                start = ent.start_char\n",
        "                end = ent.end_char\n",
        "                prior_p = get_prior_prob_bart(ent_txt, start, gen_txt)\n",
        "                post_p = get_posterior_prob_bart(\n",
        "                    ent_txt, start, end, gen_txt, src_txt_original\n",
        "                )\n",
        "\n",
        "                # Calculate Binary Overlap Feature\n",
        "                binary_overlap = (\n",
        "                    1 if ent_txt.lower().strip() in src_txt_original.lower() else 0\n",
        "                )\n",
        "\n",
        "                entity_rows.append(\n",
        "                    {\n",
        "                        \"EntryNumber\": entry_num,\n",
        "                        \"ConditionedText\": src_txt_original,  # Include for context during labeling\n",
        "                        \"GeneratedText\": gen_txt,  # Include for context during labeling\n",
        "                        \"IsHallucinated_Original\": orig_lbl,\n",
        "                        \"start\": start,\n",
        "                        \"end\": end,\n",
        "                        \"ent\": ent_txt,\n",
        "                        \"type\": ent.label_,\n",
        "                        \"prior_prob\": prior_p,  # Linear probability\n",
        "                        \"posterior_prob\": post_p,  # Linear probability\n",
        "                        \"binary_overlap\": binary_overlap,  # binary overlap\n",
        "                    }\n",
        "                )\n",
        "            del doc\n",
        "            if i > 0 and i % 5 == 0:\n",
        "                gc.collect()\n",
        "                torch.cuda.empty_cache()  # Clear cache less frequently\n",
        "        except Exception as e:\n",
        "            print(\n",
        "                f\"ERROR processing row index {i} (Entry {entry_num}) for labeling: {e}\"\n",
        "            )\n",
        "            continue\n",
        "    return entity_rows\n",
        "\n",
        "\n",
        "# Main Execution Block for Cell 2\n",
        "def generate_labeling_data():\n",
        "    csv_file = \"input.csv\"\n",
        "    N_ROWS_FOR_LABELING = 500\n",
        "    output_csv = \"entities_to_label.csv\"\n",
        "\n",
        "    print(\n",
        "        f\"Loading {N_ROWS_FOR_LABELING} rows from '{csv_file}' to generate labeling data...\"\n",
        "    )\n",
        "    try:\n",
        "        if not os.path.exists(csv_file):\n",
        "            print(f\"'{csv_file}' not found. Creating a dummy file for demonstration.\")\n",
        "            dummy_data = {\n",
        "                \"ConditionedText\": [\n",
        "                    f\"Source document {i}\" for i in range(N_ROWS_FOR_LABELING)\n",
        "                ],\n",
        "                \"GeneratedText\": [\n",
        "                    f\"Generated summary {i} with entityA and entityB.\"\n",
        "                    for i in range(N_ROWS_FOR_LABELING)\n",
        "                ],\n",
        "                \"IsHallucinated\": [\n",
        "                    \"FALSE\" if i % 2 == 0 else \"TRUE\"\n",
        "                    for i in range(N_ROWS_FOR_LABELING)\n",
        "                ],\n",
        "            }\n",
        "            pd.DataFrame(dummy_data).to_csv(csv_file, index=False)\n",
        "            print(f\"Dummy '{csv_file}' created.\")\n",
        "\n",
        "        df_labeling = pd.read_csv(csv_file, nrows=N_ROWS_FOR_LABELING)\n",
        "        if \"EntryNumber\" not in df_labeling.columns:\n",
        "            df_labeling.insert(0, \"EntryNumber\", df_labeling.index + 1)\n",
        "        print(f\"Loaded {len(df_labeling)} rows.\")\n",
        "        required_cols = [\"ConditionedText\", \"GeneratedText\", \"IsHallucinated\"]\n",
        "        if not all(col in df_labeling.columns for col in required_cols):\n",
        "            print(\n",
        "                f\"Error: Missing required columns: {required_cols}. Found: {df_labeling.columns.tolist()}\"\n",
        "            )\n",
        "            return\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{csv_file}' not found.\")\n",
        "        return\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading CSV: {e}\")\n",
        "        return\n",
        "\n",
        "    entity_labeling_list = extract_entities_for_labeling(df_labeling)\n",
        "    if not entity_labeling_list:\n",
        "        print(\"No entities extracted. Cannot create labeling file.\")\n",
        "        return\n",
        "\n",
        "    df_to_label = pd.DataFrame(entity_labeling_list)\n",
        "    print(f\"\\nExtracted {len(df_to_label)} entities for labeling.\")\n",
        "\n",
        "    try:\n",
        "        df_to_label.to_csv(output_csv, index=False)\n",
        "        print(f\"\\nSaved detailed entity data for manual labeling to '{output_csv}'.\")\n",
        "        print(\"\\n>>> ACTION REQUIRED: <<<\")\n",
        "        print(f\"1. Download '{output_csv}'.\")\n",
        "        print(\"2. Open it in a spreadsheet program.\")\n",
        "        print(\"3. Add a new column named 'ManualLabel'.\")\n",
        "        print(\n",
        "            '4. For relevant entity rows, fill ManualLabel with \"non hallucinated\", \"factual hallucination\", or \"non-factual hallucination\".'\n",
        "        )\n",
        "        print(\n",
        "            \"   (The 'binary_overlap' column is for reference/analysis and used by the model).\"\n",
        "        )\n",
        "        print(\"5. Save the modified file as 'manual_labels.csv'.\")\n",
        "        print(\"6. Upload 'manual_labels.csv' to your Colab environment.\")\n",
        "        print(\"7. Proceed to run CELL 3.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving labeling data CSV: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if device.type == \"cuda\":\n",
        "        print(\"\\nClearing GPU cache before Cell 2 execution...\")\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    if \"__file__\" not in globals():\n",
        "        generate_labeling_data()\n",
        "    if device.type == \"cuda\":\n",
        "        print(\"\\nClearing GPU cache after Cell 2 execution...\")\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R__jdTRO0R8m"
      },
      "source": [
        "CELL 3: TRAIN & EVALUATE KNN ON MANUAL LABELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoCPuz6lpOIg",
        "outputId": "872242cc-117b-4139-e3c0-93d1528cb014"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- CELL 3: TRAIN & EVALUATE KNN ON MANUAL LABELS ---\n",
            "\n",
            "--- KNN Training & Evaluation ---\n",
            "Loading manual labels from: manual_labels.csv\n",
            "Using features: ['prior_prob', 'posterior_prob', 'binary_overlap']\n",
            "Loaded 4946 valid manually labeled examples.\n",
            "Classes found in manual labels: {'Factual Hallucination': np.int64(658), 'Non Hallucinated': np.int64(3490), 'Non-Factual Hallucination': np.int64(798)}\n",
            "\n",
            "Split data into 3956 training and 990 testing examples.\n",
            "Training distribution:\n",
            "Non Hallucinated             2792\n",
            "Non-Factual Hallucination     638\n",
            "Factual Hallucination         526\n",
            "Name: count, dtype: int64\n",
            "Testing distribution:\n",
            "Non Hallucinated             698\n",
            "Non-Factual Hallucination    160\n",
            "Factual Hallucination        132\n",
            "Name: count, dtype: int64\n",
            "Features scaled.\n",
            "\n",
            "Applying SMOTE...\n",
            "Class distribution after SMOTE:\n",
            "Non Hallucinated             2792\n",
            "Non-Factual Hallucination    2792\n",
            "Factual Hallucination        2792\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Tuning KNN hyperparameters...\n",
            "Best KNN Parameters found: {'n_neighbors': 11, 'weights': 'uniform'}, Score: 0.6305\n",
            "\n",
            "--- KNN Performance Evaluation (Test Set) ---\n",
            "Using Parameters: {'n_neighbors': 11, 'weights': 'uniform'}\n",
            "Accuracy on Test Set: 0.5778\n",
            "Classification Report:\n",
            "                           precision    recall  f1-score   support\n",
            "\n",
            "    Factual Hallucination       0.16      0.33      0.22       132\n",
            "         Non Hallucinated       0.86      0.65      0.74       698\n",
            "Non-Factual Hallucination       0.40      0.46      0.43       160\n",
            "\n",
            "                 accuracy                           0.58       990\n",
            "                macro avg       0.47      0.48      0.46       990\n",
            "             weighted avg       0.69      0.58      0.62       990\n",
            "\n",
            "---------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- CELL 3: TRAIN & EVALUATE KNN ON MANUAL LABELS ---\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler  # Import Scaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import sys\n",
        "\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "\n",
        "    imblearn_installed = True\n",
        "except ImportError:\n",
        "    print(\"WARNING: 'imbalanced-learn' library not found.\")\n",
        "    print(\"         SMOTE oversampling will be skipped.\")\n",
        "    print(\n",
        "        \"         Install with: !pip install -U imbalanced-learn (run in a separate cell or add to Cell 1)\"\n",
        "    )\n",
        "    imblearn_installed = False\n",
        "\n",
        "manual_labels_file = \"manual_labels.csv\"\n",
        "test_size = 0.20  # Use 20% of manual data for testing\n",
        "k_values = [1, 3, 5, 6, 7, 8, 9, 11, 15]  # K values to test for KNN\n",
        "weights_options = [\"uniform\", \"distance\"]  # Weighting options for KNN\n",
        "\n",
        "feature_cols = [\"prior_prob\", \"posterior_prob\", \"binary_overlap\"]\n",
        "\n",
        "\n",
        "def run_cell3_evaluation():\n",
        "    print(f\"\\n--- KNN Training & Evaluation ---\")\n",
        "    print(f\"Loading manual labels from: {manual_labels_file}\")\n",
        "\n",
        "    if not os.path.exists(manual_labels_file):\n",
        "        print(\n",
        "            f\"!!! ERROR: Manual labels file '{manual_labels_file}' not found. Please upload it after labeling.\"\n",
        "        )\n",
        "        return {\"n_neighbors\": 7, \"weights\": \"uniform\"}  # Return default\n",
        "\n",
        "    try:\n",
        "        df_manual = pd.read_csv(manual_labels_file)\n",
        "    except Exception as e:\n",
        "        print(f\"!!! ERROR reading '{manual_labels_file}': {e}\")\n",
        "        return {\"n_neighbors\": 7, \"weights\": \"uniform\"}\n",
        "\n",
        "    required_manual_cols = feature_cols + [\"ManualLabel\"]\n",
        "    print(f\"Using features: {feature_cols}\")\n",
        "\n",
        "    if not all(col in df_manual.columns for col in required_manual_cols):\n",
        "        print(\n",
        "            f\"!!! ERROR: '{manual_labels_file}' missing required columns. Expected: {required_manual_cols}. Found: {df_manual.columns.tolist()}\"\n",
        "        )\n",
        "        return {\"n_neighbors\": 7, \"weights\": \"uniform\"}\n",
        "\n",
        "    df_manual = df_manual.dropna(subset=required_manual_cols).copy()\n",
        "    for col in feature_cols:\n",
        "        df_manual[col] = pd.to_numeric(df_manual[col], errors=\"coerce\")\n",
        "    df_manual = df_manual.dropna(subset=feature_cols)\n",
        "\n",
        "    if len(df_manual) < 10:\n",
        "        print(\n",
        "            f\"!!! ERROR: Insufficient valid data found ({len(df_manual)} rows). Need more labeled data.\"\n",
        "        )\n",
        "        return {\"n_neighbors\": 7, \"weights\": \"uniform\"}\n",
        "    print(f\"Loaded {len(df_manual)} valid manually labeled examples.\")\n",
        "\n",
        "    # --- Prepare Data ---\n",
        "    X_manual_all = df_manual[feature_cols].values\n",
        "    y_manual_all = df_manual[\"ManualLabel\"].values\n",
        "    labels_present, counts_present = np.unique(y_manual_all, return_counts=True)\n",
        "    label_counts_dict = dict(zip(labels_present, counts_present))\n",
        "    print(f\"Classes found in manual labels: {label_counts_dict}\")\n",
        "    min_samples_per_class = min(counts_present) if len(counts_present) > 0 else 0\n",
        "    if len(labels_present) < 2:\n",
        "        print(\"!!! WARNING: Fewer than 2 classes found. KNN may not be meaningful.\")\n",
        "\n",
        "    try:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_manual_all,\n",
        "            y_manual_all,\n",
        "            test_size=test_size,\n",
        "            random_state=42,\n",
        "            stratify=y_manual_all if min_samples_per_class >= 2 else None,\n",
        "        )\n",
        "    except ValueError as e:\n",
        "        print(\n",
        "            f\"\\nError during stratified split (min samples per class {min_samples_per_class}): {e}. Attempting unstratified split.\"\n",
        "        )\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_manual_all, y_manual_all, test_size=test_size, random_state=42\n",
        "        )\n",
        "\n",
        "    print(\n",
        "        f\"\\nSplit data into {len(y_train)} training and {len(y_test)} testing examples.\"\n",
        "    )\n",
        "    train_counts = pd.Series(y_train).value_counts()\n",
        "    test_counts = pd.Series(y_test).value_counts()\n",
        "    print(\n",
        "        f\"Training distribution:\\n{train_counts}\\nTesting distribution:\\n{test_counts}\"\n",
        "    )\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    print(\"Features scaled.\")\n",
        "\n",
        "    X_train_processed, y_train_processed = X_train_scaled, y_train\n",
        "    if imblearn_installed:\n",
        "        train_label_counts = pd.Series(y_train).value_counts()\n",
        "        min_train_samples_for_smote = (\n",
        "            train_label_counts.min() if not train_label_counts.empty else 0\n",
        "        )\n",
        "        smote_k = (\n",
        "            min(5, min_train_samples_for_smote - 1)\n",
        "            if min_train_samples_for_smote > 1\n",
        "            else 1\n",
        "        )\n",
        "        if smote_k >= 1 and len(train_label_counts) > 1:\n",
        "            try:\n",
        "                print(\"\\nApplying SMOTE...\")\n",
        "                smote = SMOTE(random_state=42, k_neighbors=smote_k)\n",
        "                X_train_processed, y_train_processed = smote.fit_resample(\n",
        "                    X_train_scaled, y_train\n",
        "                )\n",
        "                print(\n",
        "                    f\"Class distribution after SMOTE:\\n{pd.Series(y_train_processed).value_counts()}\"\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"Error during SMOTE: {e}. Using original scaled data.\")\n",
        "        else:\n",
        "            print(\"Skipping SMOTE: Not enough samples or classes in minority.\")\n",
        "    else:\n",
        "        print(\"Skipping SMOTE (imblearn not installed).\")\n",
        "\n",
        "    print(\"\\nTuning KNN hyperparameters...\")\n",
        "    param_grid = {\"n_neighbors\": k_values, \"weights\": weights_options}\n",
        "    knn_grid = KNeighborsClassifier()\n",
        "    min_processed_samples = pd.Series(y_train_processed).value_counts().min()\n",
        "    cv_folds = min(5, min_processed_samples) if min_processed_samples > 1 else 2\n",
        "\n",
        "    best_params_default = {\"n_neighbors\": 7, \"weights\": \"uniform\"}\n",
        "    best_params = best_params_default.copy()\n",
        "\n",
        "    if cv_folds < 2 or len(np.unique(y_train_processed)) < 2:\n",
        "        print(\n",
        "            f\"Warning: Cannot perform cross-validation (folds={cv_folds}, unique_classes={len(np.unique(y_train_processed))}). Using default KNN params.\"\n",
        "        )\n",
        "    else:\n",
        "        try:\n",
        "            grid_search = GridSearchCV(\n",
        "                knn_grid,\n",
        "                param_grid,\n",
        "                cv=cv_folds,\n",
        "                scoring=\"f1_macro\",\n",
        "                n_jobs=-1,\n",
        "                error_score=0.0,\n",
        "            )\n",
        "            grid_search.fit(X_train_processed, y_train_processed)\n",
        "            best_params = grid_search.best_params_\n",
        "            print(\n",
        "                f\"Best KNN Parameters found: {best_params}, Score: {grid_search.best_score_:.4f}\"\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error during GridSearchCV: {e}. Using default KNN params.\")\n",
        "\n",
        "    knn_eval = KNeighborsClassifier(**best_params)\n",
        "    knn_eval.fit(X_train_processed, y_train_processed)\n",
        "    y_pred = knn_eval.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    report_labels_unique = sorted(list(set(y_test) | set(y_pred)))\n",
        "    report = classification_report(\n",
        "        y_test,\n",
        "        y_pred,\n",
        "        labels=report_labels_unique,\n",
        "        target_names=[str(x) for x in report_labels_unique],\n",
        "        zero_division=0,\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- KNN Performance Evaluation (Test Set) ---\")\n",
        "    print(f\"Using Parameters: {best_params}\")\n",
        "    print(f\"Accuracy on Test Set: {accuracy:.4f}\\nClassification Report:\\n{report}\")\n",
        "    print(\"---------------------------------------------\")\n",
        "    del (\n",
        "        knn_eval,\n",
        "        X_train,\n",
        "        X_test,\n",
        "        y_train,\n",
        "        y_test,\n",
        "        X_manual_all,\n",
        "        y_manual_all,\n",
        "        df_manual,\n",
        "    )\n",
        "    del X_train_scaled, X_test_scaled, X_train_processed, y_train_processed\n",
        "    if \"grid_search\" in locals():\n",
        "        del grid_search\n",
        "    if \"scaler\" in locals():\n",
        "        del scaler\n",
        "    if \"smote\" in locals():\n",
        "        del smote\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    return best_params\n",
        "\n",
        "\n",
        "best_params_from_cell3 = {\"n_neighbors\": 7, \"weights\": \"uniform\"}\n",
        "if __name__ == \"__main__\":\n",
        "    if os.path.exists(manual_labels_file):\n",
        "        best_params_from_cell3 = run_cell3_evaluation()\n",
        "    else:\n",
        "        print(\n",
        "            f\"Skipping Cell 3 execution as '{manual_labels_file}' not found. Using default KNN params for Cell 4.\"\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESdjPvRN0bvd"
      },
      "source": [
        "CELL 4: APPLY TRAINED KNN TO FULL DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEO-r9RbpZl1",
        "outputId": "c1ab5743-f2e2-4fef-b7fb-c88fb001d788"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- CELL 4: APPLY TRAINED KNN TO FULL DATASET ---\n",
            "\n",
            "Loading ALL manual labels and training FINAL KNN...\n",
            "Loaded 4946 manually labeled examples for FINAL KNN training.\n",
            "Manual features scaled for final KNN. Final scaler is now fitted.\n",
            "Applying SMOTE to full manual data for final KNN training...\n",
            "Class distribution for final training after SMOTE:\n",
            "Non Hallucinated             3490\n",
            "Non-Factual Hallucination    3490\n",
            "Factual Hallucination        3490\n",
            "Name: count, dtype: int64\n",
            "FINAL KNN classifier trained with params: {'n_neighbors': 11, 'weights': 'uniform'}\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- CELL 4: APPLY TRAINED KNN TO FULL DATASET ---\")\n",
        "\n",
        "# Re-define if nlp/tokenizer/model were deleted or not in scope\n",
        "if \"nlp\" not in globals():\n",
        "    try:\n",
        "        nlp = spacy.load(\"en_core_web_md\")\n",
        "        print(\"spaCy (re)loaded for Cell 4.\")\n",
        "    except:\n",
        "        print(\"ERROR: spaCy model 'nlp' not available for Cell 4.\")\n",
        "        sys.exit(1)\n",
        "if \"tokenizer\" not in globals() or \"model\" not in globals():\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
        "        model.eval()\n",
        "        print(f\"BART model ({model_name}) (re)loaded for Cell 4.\")\n",
        "    except:\n",
        "        print(f\"ERROR: BART model not available for Cell 4.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "scaler_final = StandardScaler()\n",
        "knn_final = None\n",
        "\n",
        "# Load ALL Manual Labels & Train FINAL KNN\n",
        "print(\"\\nLoading ALL manual labels and training FINAL KNN...\")\n",
        "if not os.path.exists(manual_labels_file):\n",
        "    print(\n",
        "        f\"!!! ERROR: '{manual_labels_file}' not found. Cannot train final KNN for Cell 4.\"\n",
        "    )\n",
        "    sys.exit(1)\n",
        "try:\n",
        "    df_manual_full = pd.read_csv(manual_labels_file)\n",
        "    required_cols_full = feature_cols + [\n",
        "        \"ManualLabel\"\n",
        "    ]  # feature_cols defined in Cell 3\n",
        "    if not all(col in df_manual_full.columns for col in required_cols_full):\n",
        "        print(\n",
        "            f\"!!! ERROR: '{manual_labels_file}' missing columns for final KNN. Expected: {required_cols_full}.\"\n",
        "        )\n",
        "        sys.exit(1)\n",
        "\n",
        "    df_manual_full = df_manual_full.dropna(subset=required_cols_full).copy()\n",
        "    for col in feature_cols:\n",
        "        df_manual_full[col] = pd.to_numeric(df_manual_full[col], errors=\"coerce\")\n",
        "    df_manual_full = df_manual_full.dropna(subset=feature_cols)\n",
        "\n",
        "    if len(df_manual_full) < 3:\n",
        "        print(f\"!!! ERROR: Insufficient data in '{manual_labels_file}' for final KNN.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    X_manual_all_full = df_manual_full[feature_cols].values\n",
        "    y_manual_all_full = df_manual_full[\"ManualLabel\"].values\n",
        "    print(\n",
        "        f\"Loaded {len(y_manual_all_full)} manually labeled examples for FINAL KNN training.\"\n",
        "    )\n",
        "\n",
        "    X_manual_all_scaled_full = scaler_final.fit_transform(X_manual_all_full)\n",
        "    print(\"Manual features scaled for final KNN. Final scaler is now fitted.\")\n",
        "\n",
        "    X_train_final_processed, y_train_final_processed = (\n",
        "        X_manual_all_scaled_full,\n",
        "        y_manual_all_full,\n",
        "    )\n",
        "    if imblearn_installed:\n",
        "        counts_full = pd.Series(y_manual_all_full).value_counts()\n",
        "        min_samples_full_smote = counts_full.min() if not counts_full.empty else 0\n",
        "        smote_k_full = (\n",
        "            min(5, min_samples_full_smote - 1) if min_samples_full_smote > 1 else 1\n",
        "        )\n",
        "        if smote_k_full >= 1 and len(counts_full) > 1:\n",
        "            try:\n",
        "                print(\"Applying SMOTE to full manual data for final KNN training...\")\n",
        "                smote_final = SMOTE(random_state=42, k_neighbors=smote_k_full)\n",
        "                X_train_final_processed, y_train_final_processed = (\n",
        "                    smote_final.fit_resample(\n",
        "                        X_manual_all_scaled_full, y_manual_all_full\n",
        "                    )\n",
        "                )\n",
        "                print(\n",
        "                    f\"Class distribution for final training after SMOTE:\\n{pd.Series(y_train_final_processed).value_counts()}\"\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"Error during SMOTE for final training: {e}.\")\n",
        "        else:\n",
        "            print(\"Skipping SMOTE for final training (not enough samples/classes).\")\n",
        "\n",
        "    knn_final = KNeighborsClassifier(**best_params_from_cell3)  # Use params from Cell 3\n",
        "    knn_final.fit(X_train_final_processed, y_train_final_processed)\n",
        "    print(f\"FINAL KNN classifier trained with params: {best_params_from_cell3}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"!!! ERROR during final KNN training setup: {e}\")\n",
        "    sys.exit(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kjAdUlq0hzc"
      },
      "source": [
        "Entity Labeling Function - USES FINAL KNN & SCALER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcxp1_rhpea2"
      },
      "outputs": [],
      "source": [
        "def label_entity_prob_final(\n",
        "    src_txt: str, gen_txt: str, ent_obj, scaler, knn_model\n",
        ") -> tuple[str, float, float, int]:\n",
        "    ent_txt = ent_obj.text\n",
        "    start = ent_obj.start_char\n",
        "    end = ent_obj.end_char\n",
        "    prior_p = get_prior_prob_bart(ent_txt, start, gen_txt)\n",
        "    post_p = get_posterior_prob_bart(ent_txt, start, end, gen_txt, src_txt)\n",
        "    binary_overlap_val = 1 if ent_txt.lower().strip() in src_txt.lower() else 0\n",
        "\n",
        "    features_raw = np.array([[prior_p, post_p, binary_overlap_val]])\n",
        "    features_raw = np.nan_to_num(features_raw, nan=0.0, posinf=1.0, neginf=0.0)\n",
        "    features_scaled = scaler.transform(features_raw)\n",
        "    pred = \"error\"\n",
        "    try:\n",
        "        if knn_model is None:\n",
        "            print(\"KNN Prediction Error: knn_final model is not trained/available.\")\n",
        "        else:\n",
        "            pred = knn_model.predict(features_scaled)[0]\n",
        "    except Exception as e:\n",
        "        print(\n",
        "            f\"KNN Prediction Error: {e}, Raw: {features_raw}, Scaled: {features_scaled}\"\n",
        "        )\n",
        "    return str(pred), prior_p, post_p, binary_overlap_val\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2RCiNAQ0oYg"
      },
      "source": [
        "Entity Extraction Function - USES FINAL KNN & SCALER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnznjzcbpl5s"
      },
      "outputs": [],
      "source": [
        "def extract_and_aggregate_entities_final(df, scaler, knn_model):\n",
        "    aggregated_rows = []\n",
        "    print(\n",
        "        f\"\\nExtracting, classifying entities, and aggregating for {len(df)} rows (Cell 4)...\"\n",
        "    )\n",
        "    if scaler is None or not hasattr(scaler, \"mean_\"):\n",
        "        print(\"Error: Final scaler not fitted.\")\n",
        "        return []\n",
        "    if knn_model is None:\n",
        "        print(\"Error: Final KNN model not trained.\")\n",
        "        return []\n",
        "\n",
        "    row_iterator = tqdm(\n",
        "        df.iterrows(), total=len(df), desc=\"Processing Full Dataset (Cell 4)\"\n",
        "    )\n",
        "    for i, row in row_iterator:\n",
        "        src_txt = (\n",
        "            str(row[\"ConditionedText\"]) if pd.notna(row[\"ConditionedText\"]) else \"\"\n",
        "        )\n",
        "        gen_txt = str(row[\"GeneratedText\"]) if pd.notna(row[\"GeneratedText\"]) else \"\"\n",
        "        orig_lbl = row[\"IsHallucinated\"]\n",
        "        entry_num = row.get(\"EntryNumber\", i + 1)\n",
        "\n",
        "        entities_in_row = {\n",
        "            \"EntryNumber\": entry_num,\n",
        "            \"ConditionedText\": src_txt,\n",
        "            \"GeneratedText\": gen_txt,\n",
        "            \"IsHallucinated_Original\": orig_lbl,\n",
        "            \"ent\": [],\n",
        "            \"type\": [],\n",
        "            \"start\": [],\n",
        "            \"end\": [],\n",
        "            \"label_pred\": [],\n",
        "            \"prior_pred_val\": [],\n",
        "            \"posterior_pred_val\": [],\n",
        "            \"binary_overlap_pred_val\": [],\n",
        "        }\n",
        "        try:\n",
        "            max_spacy_len = nlp.max_length\n",
        "            doc = (\n",
        "                nlp(gen_txt[: max_spacy_len - 1], disable=[\"parser\"])\n",
        "                if len(gen_txt) >= max_spacy_len\n",
        "                else nlp(gen_txt, disable=[\"parser\"])\n",
        "            )\n",
        "            if doc.ents:\n",
        "                for ent_obj in doc.ents:\n",
        "                    lbl, p_p, post_p, bo_val = label_entity_prob_final(\n",
        "                        src_txt, gen_txt, ent_obj, scaler, knn_model\n",
        "                    )\n",
        "                    entities_in_row[\"ent\"].append(ent_obj.text)\n",
        "                    entities_in_row[\"type\"].append(ent_obj.label_)\n",
        "                    entities_in_row[\"start\"].append(ent_obj.start_char)\n",
        "                    entities_in_row[\"end\"].append(ent_obj.end_char)\n",
        "                    entities_in_row[\"label_pred\"].append(lbl)\n",
        "                    entities_in_row[\"prior_pred_val\"].append(p_p)\n",
        "                    entities_in_row[\"posterior_pred_val\"].append(post_p)\n",
        "                    entities_in_row[\"binary_overlap_pred_val\"].append(bo_val)\n",
        "            aggregated_rows.append(entities_in_row)\n",
        "            del doc\n",
        "            if i > 0 and i % 10 == 0:\n",
        "                gc.collect()\n",
        "                torch.cuda.empty_cache()\n",
        "        except Exception as e:\n",
        "            print(\n",
        "                f\"ERROR processing row {i} (Entry {entry_num}) in Cell 4 extract: {e}\"\n",
        "            )\n",
        "            entities_in_row.update(\n",
        "                {k: [] for k in entities_in_row if isinstance(entities_in_row[k], list)}\n",
        "            )\n",
        "            aggregated_rows.append(entities_in_row)\n",
        "            continue\n",
        "    return aggregated_rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2tGHugr0uVW"
      },
      "source": [
        "Main Execution Block for Cell 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8e011ac8f7354b339e6e8895eb8856d1",
            "a4b9d1fc15734cafa5eb701ff016d351",
            "47c85ef472c147888f8ab9904dabf027",
            "032673cd0ee74b7f801b30c02c1ce458",
            "263e3f6db2d6479daf3da70b80291afb",
            "795f582e5ee948e7bb9c64e69abf479f",
            "2dadb6eb3eff4d4d8eea735a4dc882a6",
            "566f9291caa14c79a4d5574f1538e883",
            "6be52b64ed3742edb57ebc461895948e",
            "1adf35ef82c8478ba59e13928fb5c4ad",
            "b9bb45e9286047deb815615dfe457167"
          ]
        },
        "id": "K9Mj-ZcHpq4x",
        "outputId": "21057c26-d2ac-4546-f2fe-0473816ebd50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Clearing GPU cache before Cell 4 main execution...\n",
            "\n",
            "Loading ALL rows from 'input.csv' (all columns) for final classification...\n",
            "Loaded 2000 rows for final processing.\n",
            "\n",
            "Extracting, classifying entities, and aggregating for 2000 rows (Cell 4)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e011ac8f7354b339e6e8895eb8856d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Full Dataset (Cell 4):   0%|          | 0/2000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Saved FINAL FILTERED & AGGREGATED entity analysis to 'entities_analysis_final.csv'.\n",
            "   EntryNumber  \\\n",
            "0            1   \n",
            "2            3   \n",
            "3            4   \n",
            "5            6   \n",
            "6            7   \n",
            "\n",
            "                                                                                                   ent  \\\n",
            "0                                                 [DUI, Two thirds, first, Taiwan, DUI, DUI, DUI, DUI]   \n",
            "2                                                                                           [73%, TNF]   \n",
            "3                                                                                              [Birds]   \n",
            "5                                                                 [Two, EVT, Gated Recurrent Unit GRU]   \n",
            "6  [1, Methylpyrene, CYP, 2, Methylpyrene, 3, Methylpyrene, Methylpyrene, as low as 0.125, V79, Met...   \n",
            "\n",
            "                                                                                            label_pred  \\\n",
            "0  [Non-Factual Hallucination, Non Hallucinated, Non Hallucinated, Non-Factual Hallucination, Non H...   \n",
            "2                                                   [Non-Factual Hallucination, Factual Hallucination]   \n",
            "3                                                                                   [Non Hallucinated]   \n",
            "5                                 [Non Hallucinated, Factual Hallucination, Non-Factual Hallucination]   \n",
            "6  [Non Hallucinated, Non Hallucinated, Non Hallucinated, Non-Factual Hallucination, Non-Factual Ha...   \n",
            "\n",
            "             binary_overlap_pred_val  \n",
            "0           [1, 1, 1, 0, 1, 1, 1, 1]  \n",
            "2                             [0, 0]  \n",
            "3                                [1]  \n",
            "5                          [1, 0, 0]  \n",
            "6  [1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1]  \n",
            "\n",
            "--- Performing FINAL Row-level Classification ---\n",
            "\n",
            "Saved FINAL dataset to 'updated_dataset_final.csv'.\n",
            "   EntryNumber                                    ConditionedText  \\\n",
            "0            1  Driving riding under the influence of alcohol ...   \n",
            "1            2  Sleep problems are common in school age childr...   \n",
            "2            3  Elevated inflammatory cytokines contribute to ...   \n",
            "3            4  Assessing local habitat quality via social cue...   \n",
            "4            5  Random parameters model has been demonstrated ...   \n",
            "5            6  Pedestrian safety plays an important role in t...   \n",
            "6            7  1 Methylpyrene is a ubiquitous environmental p...   \n",
            "7            8  Lung cancer is one of the leading causes of ca...   \n",
            "8            9  Eugenol a phenylpropanoid predominantly found ...   \n",
            "9           10  Clopidogrel a clinically used antiplatelet age...   \n",
            "\n",
            "                                       GeneratedText  IsHallucinated  \\\n",
            "0  Distinct drinking profiles of DUI for motorcyc...           False   \n",
            "1  Nonpharmacological sleep interventions effecti...            True   \n",
            "2  Elevated acetylcholinesterase activity promote...            True   \n",
            "3  Birds may use song features from heterospecifi...           False   \n",
            "4  Random parameters model effectively captures u...            True   \n",
            "5  Automated computer vision techniques are used ...           False   \n",
            "6  1 Methylpyrene induces micronuclei formation i...            True   \n",
            "7  Cytotoxic effects of TMBP on A549 human non-sm...            True   \n",
            "8  Establishing catenin as a novel druggable targ...           False   \n",
            "9  Clopidogrel hydrolysis rates vary significantl...            True   \n",
            "\n",
            "  IsFactual_pred  \n",
            "0           True  \n",
            "1          False  \n",
            "2          False  \n",
            "3           True  \n",
            "4          False  \n",
            "5           True  \n",
            "6          False  \n",
            "7          False  \n",
            "8           True  \n",
            "9          False  \n",
            "\n",
            "===============================================\n",
            "Processing Complete (Cell 4).\n",
            "Final results saved to 'entities_analysis_final.csv' and 'updated_dataset_final.csv'.\n",
            "===============================================\n",
            "\n",
            "Clearing GPU cache after Cell 4 main execution...\n"
          ]
        }
      ],
      "source": [
        "def apply_final_classifier():\n",
        "    csv_file_full = \"input.csv\"\n",
        "    N_ROWS_TO_PROCESS_FULL = None\n",
        "\n",
        "    load_msg = (\n",
        "        \"ALL rows\"\n",
        "        if N_ROWS_TO_PROCESS_FULL is None\n",
        "        else f\"{N_ROWS_TO_PROCESS_FULL} rows\"\n",
        "    )\n",
        "    print(\n",
        "        f\"\\nLoading {load_msg} from '{csv_file_full}' (all columns) for final classification...\"\n",
        "    )\n",
        "    try:\n",
        "        df_full = pd.read_csv(csv_file_full, nrows=N_ROWS_TO_PROCESS_FULL)\n",
        "        if \"EntryNumber\" not in df_full.columns:\n",
        "            df_full.insert(0, \"EntryNumber\", df_full.index + 1)\n",
        "        print(f\"Loaded {len(df_full)} rows for final processing.\")\n",
        "        if df_full.empty:\n",
        "            print(\"Error: Loaded DataFrame for final processing is empty.\")\n",
        "            return\n",
        "        req_cols_full_load = [\"ConditionedText\", \"GeneratedText\", \"IsHallucinated\"]\n",
        "        if not all(col in df_full.columns for col in req_cols_full_load):\n",
        "            print(\n",
        "                f\"Error: Missing required columns in '{csv_file_full}'. Expected: {req_cols_full_load}.\"\n",
        "            )\n",
        "            return\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading CSV for final processing: {e}\")\n",
        "        return\n",
        "\n",
        "    if \"scaler_final\" not in globals() or not hasattr(scaler_final, \"mean_\"):\n",
        "        print(\"Error: scaler_final not fitted. Cannot apply classifier.\")\n",
        "        return\n",
        "    if \"knn_final\" not in globals() or knn_final is None:\n",
        "        print(\"Error: knn_final model not trained. Cannot apply classifier.\")\n",
        "        return\n",
        "\n",
        "    aggregated_entity_data_list_final = extract_and_aggregate_entities_final(\n",
        "        df_full, scaler_final, knn_final\n",
        "    )\n",
        "    if not aggregated_entity_data_list_final:\n",
        "        print(\"Final entity extraction/aggregation failed or returned empty list.\")\n",
        "        return\n",
        "    df_entities_aggregated_final = pd.DataFrame(aggregated_entity_data_list_final)\n",
        "\n",
        "    entity_output_file = \"entities_analysis_final.csv\"\n",
        "    if not df_entities_aggregated_final.empty:\n",
        "        try:\n",
        "            df_entities_filtered_final = df_entities_aggregated_final[\n",
        "                df_entities_aggregated_final[\"ent\"].apply(\n",
        "                    lambda x: isinstance(x, list) and len(x) > 0\n",
        "                )\n",
        "            ].copy()\n",
        "            entity_cols_to_save = [\n",
        "                \"EntryNumber\",\n",
        "                \"ConditionedText\",\n",
        "                \"GeneratedText\",\n",
        "                \"IsHallucinated_Original\",\n",
        "                \"ent\",\n",
        "                \"type\",\n",
        "                \"start\",\n",
        "                \"end\",\n",
        "                \"label_pred\",\n",
        "                \"prior_pred_val\",\n",
        "                \"posterior_pred_val\",\n",
        "                \"binary_overlap_pred_val\",  # Added new column\n",
        "            ]\n",
        "            entity_cols_to_save = [\n",
        "                col\n",
        "                for col in entity_cols_to_save\n",
        "                if col in df_entities_filtered_final.columns\n",
        "            ]\n",
        "            if not df_entities_filtered_final.empty:\n",
        "                df_entities_filtered_final[entity_cols_to_save].to_csv(\n",
        "                    entity_output_file, index=False\n",
        "                )\n",
        "                print(\n",
        "                    f\"\\nSaved FINAL FILTERED & AGGREGATED entity analysis to '{entity_output_file}'.\"\n",
        "                )\n",
        "                preview_cols_agg = [\n",
        "                    c\n",
        "                    for c in [\n",
        "                        \"EntryNumber\",\n",
        "                        \"ent\",\n",
        "                        \"label_pred\",\n",
        "                        \"binary_overlap_pred_val\",\n",
        "                    ]\n",
        "                    if c in df_entities_filtered_final.columns\n",
        "                ]\n",
        "                with pd.option_context(\"display.max_colwidth\", 100):\n",
        "                    print(df_entities_filtered_final[preview_cols_agg].head())\n",
        "            else:\n",
        "                print(\n",
        "                    \"\\nNo rows with detected entities found after filtering for final analysis.\"\n",
        "                )\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving final aggregated entity analysis CSV: {e}\")\n",
        "    else:\n",
        "        print(\"\\nNo entities were extracted/processed in the final run.\")\n",
        "\n",
        "    print(\"\\n--- Performing FINAL Row-level Classification ---\")\n",
        "    df_full[\"IsFactual_pred\"] = \"False\"\n",
        "    if not df_entities_aggregated_final.empty:\n",
        "        row_predictions = {}\n",
        "        for _, agg_row in df_entities_aggregated_final.iterrows():\n",
        "            entry_num = agg_row[\"EntryNumber\"]\n",
        "            labels = agg_row.get(\"label_pred\", [])\n",
        "            valid_labels = [\n",
        "                lbl for lbl in labels if isinstance(lbl, str) and lbl != \"error\"\n",
        "            ]\n",
        "            original_halluc_label = str(agg_row[\"IsHallucinated_Original\"]).upper()\n",
        "            if original_halluc_label == \"FALSE\":\n",
        "                row_predictions[entry_num] = \"True\"\n",
        "            else:\n",
        "                row_predictions[entry_num] = (\n",
        "                    \"True\"\n",
        "                    if (\n",
        "                        valid_labels\n",
        "                        and all(label == \"non hallucinated\" for label in valid_labels)\n",
        "                    )\n",
        "                    else \"False\"\n",
        "                )\n",
        "        df_full[\"IsFactual_pred\"] = df_full[\"EntryNumber\"].map(row_predictions)\n",
        "        for index, row_df in df_full.loc[df_full[\"IsFactual_pred\"].isna()].iterrows():\n",
        "            df_full.loc[index, \"IsFactual_pred\"] = (\n",
        "                \"True\" if str(row_df[\"IsHallucinated\"]).upper() == \"FALSE\" else \"False\"\n",
        "            )\n",
        "        df_full[\"IsFactual_pred\"] = df_full[\"IsFactual_pred\"].fillna(\n",
        "            \"Error - Prediction Missing\"\n",
        "        )\n",
        "    else:\n",
        "        print(\n",
        "            \"No aggregated entity data for final row predictions. Basing on original 'IsHallucinated'.\"\n",
        "        )\n",
        "        df_full[\"IsFactual_pred\"] = df_full[\"IsHallucinated\"].apply(\n",
        "            lambda x: \"True\" if str(x).upper() == \"FALSE\" else \"False\"\n",
        "        )\n",
        "\n",
        "    row_output_file_final = \"updated_dataset_final.csv\"\n",
        "    try:\n",
        "        df_full.to_csv(row_output_file_final, index=False)\n",
        "        print(f\"\\nSaved FINAL dataset to '{row_output_file_final}'.\")\n",
        "        preview_cols_row = [\n",
        "            c\n",
        "            for c in [\n",
        "                \"EntryNumber\",\n",
        "                \"ConditionedText\",\n",
        "                \"GeneratedText\",\n",
        "                \"IsHallucinated\",\n",
        "                \"IsFactual_pred\",\n",
        "            ]\n",
        "            if c in df_full.columns\n",
        "        ]\n",
        "        print(df_full[preview_cols_row].head(10))\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving final dataset CSV: {e}\")\n",
        "\n",
        "    print(\"\\n===============================================\")\n",
        "    print(\"Processing Complete (Cell 4).\")\n",
        "    print(\n",
        "        f\"Final results saved to '{entity_output_file}' and '{row_output_file_final}'.\"\n",
        "    )\n",
        "    print(\"===============================================\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if (\n",
        "        \"scaler_final\" not in globals()\n",
        "        or not hasattr(scaler_final, \"mean_\")\n",
        "        or \"knn_final\" not in globals()\n",
        "        or knn_final is None\n",
        "    ):\n",
        "        print(\n",
        "            \"WARNING: Final scaler or KNN model not ready before apply_final_classifier. This might indicate an issue if previous Cell 4 blocks didn't run successfully.\"\n",
        "        )\n",
        "\n",
        "    if device.type == \"cuda\":\n",
        "        print(\"\\nClearing GPU cache before Cell 4 main execution...\")\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "    if \"__file__\" not in globals():\n",
        "        apply_final_classifier()\n",
        "    if device.type == \"cuda\":\n",
        "        print(\"\\nClearing GPU cache after Cell 4 main execution...\")\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "032673cd0ee74b7f801b30c02c1ce458": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1adf35ef82c8478ba59e13928fb5c4ad",
            "placeholder": "​",
            "style": "IPY_MODEL_b9bb45e9286047deb815615dfe457167",
            "value": " 2000/2000 [10:03&lt;00:00,  4.47it/s]"
          }
        },
        "1adf35ef82c8478ba59e13928fb5c4ad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "263e3f6db2d6479daf3da70b80291afb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28325e6046dd4b80939649b0557542ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2dadb6eb3eff4d4d8eea735a4dc882a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2fdded26e124457c82e7154c28f065ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33663a7b75414e0a8e3f59e906a46ac7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4487f56314354bc4a98cce3430e2c252": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47c85ef472c147888f8ab9904dabf027": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_566f9291caa14c79a4d5574f1538e883",
            "max": 2000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6be52b64ed3742edb57ebc461895948e",
            "value": 2000
          }
        },
        "566f9291caa14c79a4d5574f1538e883": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62e452dfa4364736b6cf901b024f4850": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4487f56314354bc4a98cce3430e2c252",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_28325e6046dd4b80939649b0557542ec",
            "value": 500
          }
        },
        "68e84dba135f4b61bcf80208ccc21a78": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6be52b64ed3742edb57ebc461895948e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "795f582e5ee948e7bb9c64e69abf479f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b9fa13ed29440d09db352c63c3f126e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6f0a3ca3f3348ef953af6fd4c338099",
            "placeholder": "​",
            "style": "IPY_MODEL_e63b23ff0b2641a69061e409de39e1d4",
            "value": "Processing Rows for Labeling: 100%"
          }
        },
        "8e011ac8f7354b339e6e8895eb8856d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a4b9d1fc15734cafa5eb701ff016d351",
              "IPY_MODEL_47c85ef472c147888f8ab9904dabf027",
              "IPY_MODEL_032673cd0ee74b7f801b30c02c1ce458"
            ],
            "layout": "IPY_MODEL_263e3f6db2d6479daf3da70b80291afb"
          }
        },
        "a2485beeb6064a5d927d6edcb803b8fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68e84dba135f4b61bcf80208ccc21a78",
            "placeholder": "​",
            "style": "IPY_MODEL_33663a7b75414e0a8e3f59e906a46ac7",
            "value": " 500/500 [02:53&lt;00:00,  2.51it/s, EntitiesFound=0]"
          }
        },
        "a4b9d1fc15734cafa5eb701ff016d351": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_795f582e5ee948e7bb9c64e69abf479f",
            "placeholder": "​",
            "style": "IPY_MODEL_2dadb6eb3eff4d4d8eea735a4dc882a6",
            "value": "Processing Full Dataset (Cell 4): 100%"
          }
        },
        "b9bb45e9286047deb815615dfe457167": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e63b23ff0b2641a69061e409de39e1d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0192cee25b24b91b33a13082bbcb042": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7b9fa13ed29440d09db352c63c3f126e",
              "IPY_MODEL_62e452dfa4364736b6cf901b024f4850",
              "IPY_MODEL_a2485beeb6064a5d927d6edcb803b8fe"
            ],
            "layout": "IPY_MODEL_2fdded26e124457c82e7154c28f065ca"
          }
        },
        "f6f0a3ca3f3348ef953af6fd4c338099": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
