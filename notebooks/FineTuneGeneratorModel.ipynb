{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"DATASET_NAME = \"TRnlp/MixSub\"\nMODEL_NAME = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\nTRAINED_MODEL_NAME = \"Llama-3.2-1B-Instruct-bnb-4bit-MixSub\"\nTRAINED_MODEL_REPO = f\"AdityaMayukhSom/{TRAINED_MODEL_NAME}\"\nMAX_SEQ_LEN = 2048\nLOAD_IN_4BIT = True\nDTYPE = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:29:12.835239Z","iopub.execute_input":"2025-03-10T03:29:12.835560Z","iopub.status.idle":"2025-03-10T03:29:12.839821Z","shell.execute_reply.started":"2025-03-10T03:29:12.835537Z","shell.execute_reply":"2025-03-10T03:29:12.838916Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login, create_repo\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\nlogin(token=hf_token)\n# create_repo(TRAINED_MODEL_REPO)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:29:13.101407Z","iopub.execute_input":"2025-03-10T03:29:13.101704Z","iopub.status.idle":"2025-03-10T03:29:13.254199Z","shell.execute_reply.started":"2025-03-10T03:29:13.101682Z","shell.execute_reply":"2025-03-10T03:29:13.253246Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom pathlib import Path\nfrom datasets import load_dataset, load_from_disk, Dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:29:13.306004Z","iopub.execute_input":"2025-03-10T03:29:13.306329Z","iopub.status.idle":"2025-03-10T03:29:13.310702Z","shell.execute_reply.started":"2025-03-10T03:29:13.306306Z","shell.execute_reply":"2025-03-10T03:29:13.309265Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from packaging.version import Version as V\n\ntry:\n    import torch\n    from torch.version import cuda\nexcept Exception as e:\n    raise ImportError(\"Install torch via `pip install torch`\")\n\nv = V(torch.__version__)\nis_ampere = torch.cuda.get_device_capability()[0] >= 8\nxformers = \"xformers==0.0.27\" if v < V(\"2.4.0\") else \"xformers\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nif cuda != \"12.1\" and cuda != \"11.8\" and cuda != \"12.4\":\n    raise RuntimeError(f\"CUDA = {cuda} not supported!\")\nif   v <= V('2.1.0'):\n    raise RuntimeError(f\"Torch = {v} too old!\")\nelif v <= V('2.1.1'):\n    x = 'cu{}{}-torch211'\nelif v <= V('2.1.2'):\n    x = 'cu{}{}-torch212'\nelif v  < V('2.3.0'):\n    x = 'cu{}{}-torch220'\nelif v  < V('2.4.0'):\n    x = 'cu{}{}-torch230'\nelif v  < V('2.5.0'):\n    x = 'cu{}{}-torch240'\nelif v  < V('2.6.0'):\n    x = 'cu{}{}-torch250'\nelse:\n    raise RuntimeError(f\"Torch = {v} too new!\")\n\nx = x.format(cuda.replace(\".\", \"\"), \"-ampere\" if is_ampere else \"\")\nprint(f'pip install --upgrade pip && pip install \"unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git\"')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:29:13.510530Z","iopub.execute_input":"2025-03-10T03:29:13.510900Z","iopub.status.idle":"2025-03-10T03:29:13.519811Z","shell.execute_reply.started":"2025-03-10T03:29:13.510871Z","shell.execute_reply":"2025-03-10T03:29:13.518682Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n!pip install --upgrade pip\n!pip install --no-deps {xformers} trl peft accelerate bitsandbytes triton\n!pip install \"unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:29:13.660024Z","iopub.execute_input":"2025-03-10T03:29:13.660343Z","iopub.status.idle":"2025-03-10T03:29:32.147024Z","shell.execute_reply.started":"2025-03-10T03:29:13.660318Z","shell.execute_reply":"2025-03-10T03:29:32.145955Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Reference Links For Fine Tunning Llama 3.2 With Unsloth** \n\n1. [Fine-tuning Llama 3.2 Using Unsloth](https://www.kdnuggets.com/fine-tuning-llama-using-unsloth)\n2. [Fine-tuning Llama 3 with Unsloth: A Beginnerâ€™s Guide](https://medium.com/@seekmeai/fine-tuning-llama-3-with-unsloth-a-beginners-guide-d239d48eaf71)","metadata":{}},{"cell_type":"code","source":"from unsloth import FastLanguageModel\n\nfast_language_model, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = MODEL_NAME,\n    max_seq_length = MAX_SEQ_LEN,\n    dtype = DTYPE,\n    load_in_4bit = LOAD_IN_4BIT\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:29:32.148406Z","iopub.execute_input":"2025-03-10T03:29:32.148724Z","iopub.status.idle":"2025-03-10T03:29:44.029995Z","shell.execute_reply.started":"2025-03-10T03:29:32.148691Z","shell.execute_reply":"2025-03-10T03:29:44.029246Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    fast_language_model, \n    r = 16,\n    target_modules = [\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"up_proj\",\n        \"down_proj\",\n        \"gate_proj\",\n    ],\n    lora_alpha = 16,\n    lora_dropout = 0,\n    bias = \"none\",\n    use_gradient_checkpointing = \"unsloth\",\n    random_state = 69,\n    use_rslora = False,\n    loftq_config = None,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:29:44.031442Z","iopub.execute_input":"2025-03-10T03:29:44.031710Z","iopub.status.idle":"2025-03-10T03:29:50.946469Z","shell.execute_reply.started":"2025-03-10T03:29:44.031676Z","shell.execute_reply":"2025-03-10T03:29:50.945564Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# https://huggingface.co/docs/datasets/en/loading#hugging-face-hub\ndataset = load_dataset(DATASET_NAME)\n\n# Changing all the column names to have uniform singular forms\n# All column names are now in singular form\ndataset = dataset.rename_column(\"Highlights\", \"Highlight\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:29:50.947939Z","iopub.execute_input":"2025-03-10T03:29:50.948195Z","iopub.status.idle":"2025-03-10T03:29:51.767807Z","shell.execute_reply.started":"2025-03-10T03:29:50.948172Z","shell.execute_reply":"2025-03-10T03:29:51.765962Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define how many examples to use for training and evaluation during setup\nTRAIN_DATASET_LENGTH = 10\nEVAL_DATASET_LENGTH = 5\n\n# select less number of examples for training, and even less for testing, \n# if everything goes well,  we can fine tune on a larger dataset\ntrain_dataset = dataset[\"train\"].select(range(TRAIN_DATASET_LENGTH))\neval_dataset = dataset[\"test\"].select(range(EVAL_DATASET_LENGTH))\n\n# Check train dataset before appending 'Prompt' column\n# train_dataset.to_pandas().head()\n# eval_dataset.to_pandas().head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"INSTRUCTIONS = \"\"\"\nYou are instructed to generate a scientifically accurate highlight of the provided passage without additional \nsentences such as headings or introductions before or after the generated text as it will be used as summary \nin a custom dataset. The highlight should sound plausible and should not contain incorrect information. Generate \n3-5 concise highlight points from the provided research paper abstract, covering key contributions, methods and \noutcomes. Each point should contain 10 to 15 words only. Return the points in plain text format without bullets.\n\nNo Additional Commentary: Exclude lines like \"Here are 3-5 concise highlight points\".\n\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token\n\ndef format_abstract_highlight_as_prompt(examples: list):  \n    prompts: list[str] = []\n\n    abstracts = examples[\"Abstract\"]\n    highlights = examples['Highlight']\n    \n    for abstract, highlight in zip(abstracts, highlights):\n        row_json = [\n            {\"role\": \"system\", \"content\": INSTRUCTIONS},\n            {\"role\": \"user\", \"content\": abstract},\n            # Must add EOS_TOKEN, otherwise your generation will go on forever!\n            {\"role\": \"assistant\", \"content\": highlight + EOS_TOKEN}\n        ]\n        \n        prompt = tokenizer.apply_chat_template(\n            row_json, \n            tokenize=False, \n            add_generation_prompt=False,\n            return_tensors=\"pt\"\n        )\n\n        prompts.append(prompt)\n        \n    return { \n        \"Prompt\": prompts,\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:29:51.768974Z","iopub.execute_input":"2025-03-10T03:29:51.769256Z","iopub.status.idle":"2025-03-10T03:29:52.760917Z","shell.execute_reply.started":"2025-03-10T03:29:51.769231Z","shell.execute_reply":"2025-03-10T03:29:52.759988Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Append Prompt column on which the model will be trained\ntrain_dataset = train_dataset.map(format_abstract_highlight_as_prompt, batched=True)\neval_dataset = eval_dataset.map(format_abstract_highlight_as_prompt, batched=True) \n\n# Check train dataset after adding 'Prompt' column\n# train_dataset.to_pandas().head()\n# eval_dataset.to_pandas().head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:29:52.761828Z","iopub.execute_input":"2025-03-10T03:29:52.762068Z","iopub.status.idle":"2025-03-10T03:29:53.243009Z","shell.execute_reply.started":"2025-03-10T03:29:52.762048Z","shell.execute_reply":"2025-03-10T03:29:53.241980Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset[0]['Prompt']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:29:53.243791Z","iopub.execute_input":"2025-03-10T03:29:53.244025Z","iopub.status.idle":"2025-03-10T03:29:53.249440Z","shell.execute_reply.started":"2025-03-10T03:29:53.244005Z","shell.execute_reply":"2025-03-10T03:29:53.248735Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Setup Model Training And Evaluation Pipeline","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom unsloth import is_bfloat16_supported\nfrom unsloth.chat_templates import train_on_responses_only\nfrom transformers import TrainingArguments, DataCollatorForSeq2Seq","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:29:53.251488Z","iopub.execute_input":"2025-03-10T03:29:53.251798Z","iopub.status.idle":"2025-03-10T03:29:53.261431Z","shell.execute_reply.started":"2025-03-10T03:29:53.251746Z","shell.execute_reply":"2025-03-10T03:29:53.260537Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Setup The Evaluation Metrics Calculator Function\n\n[Reference To Calculate ROUGE Metrics](https://medium.com/@MUmarAmanat/llm-evaluation-with-rouge-0ebf6cf2aed4)","metadata":{}},{"cell_type":"code","source":"import nltk\nimport evaluate\nfrom nltk.translate.bleu_score import sentence_bleu\n\nrouge = evaluate.load('rouge')\n\ndef compute_metrics(pred):\n    print(pred)\n    \n    references = pred.label_ids\n    generated_texts = pred.predictions\n    \n    bleu_scores = []\n    for reference, generated_text in zip(references, generated_texts):\n        reference_text = train_dataset[reference]['text']\n        bleu_score = sentence_bleu([reference_text], generated_text)\n        bleu_scores.append(bleu_score)\n\n\n    rouge_scores = rouge.compute(\n        predictions=original_model_summaries, \n        references=human_baseline_summaries[0: len(original_model_summaries)],\n        use_aggregator=True\n    )\n\n    return {\n        'bleu': sum(bleu_scores) / len(bleu_scores),\n        'rouge1': rouge_scores['rouge1'],\n        'rouge2': rouge_scores['rouge2'],\n        'rougeL': rouge_scores['rougeL'],\n        'rougeLsum': rouge_scores['rougeLsum']\n        \n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Setup Hyperparameters\n\nWe need to specifically check what is the optimal train and eval batch size, to optimally utilize the GPU, so that the kaggle runtime doesn't expire or crash.","metadata":{}},{"cell_type":"code","source":"PER_DEVICE_TRAIN_BATCH_SIZE = 2\nPER_DEVICE_EVAL_BATCH_SIZE = 2\nGRADIENT_ACCUMULATION_STEPS = 4\nWARMUP_STEPS = 5\nSAVE_STEPS = 500\nMAX_STEPS = 60\nNUM_TRAIN_EPOCHS = 3\nSAVE_TOTAL_LIMIT = 2\nLEARNING_RATE = 2e-4\nWEIGHT_DECAY = 0.01","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_args = TrainingArguments(\n    per_device_train_batch_size = PER_DEVICE_TRAIN_BATCH_SIZE,\n    per_device_eval_batch_size = PER_DEVICE_EVAL_BATCH_SIZE,\n    gradient_accumulation_steps = GRADIENT_ACCUMULATION_STEPS,\n    warmup_steps = WARMUP_STEPS,\n    eval_strategy=\"steps\",\n    eval_steps = 0.2,\n    num_train_epochs = NUM_TRAIN_EPOCHS, # Set this to 1 for one full training run\n    save_total_limit = SAVE_TOTAL_LIMIT,\n    save_steps = SAVE_STEPS,\n    max_steps = MAX_STEPS,\n    learning_rate = LEARNING_RATE,\n    fp16 = not is_bfloat16_supported(),\n    bf16 = is_bfloat16_supported(),\n    optim = \"adamw_8bit\",\n    weight_decay = WEIGHT_DECAY,\n    lr_scheduler_type = \"linear\",\n    seed = 3407,\n    output_dir = TRAINED_MODEL_NAME,\n    report_to = \"none\",\n    load_best_model_at_end=True,\n    push_to_hub=True\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = train_dataset,\n    eval_dataset = eval_dataset,\n    dataset_text_field = \"Prompt\", # The field on which to train the model, we have added the generated prompt under 'Prompt' header\n    max_seq_length = MAX_SEQ_LEN,\n    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n    dataset_num_proc = 2,\n    packing = False,\n    args = training_args,\n    compute_metrics = compute_metrics\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:29:53.262638Z","iopub.execute_input":"2025-03-10T03:29:53.262913Z","iopub.status.idle":"2025-03-10T03:29:57.311047Z","shell.execute_reply.started":"2025-03-10T03:29:53.262887Z","shell.execute_reply":"2025-03-10T03:29:57.309945Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = train_on_responses_only(\n    trainer,\n    instruction_part = \"<|start_header_id|>system<|end_header_id|>\",\n    response_part = \"<|start_header_id|>assistant<|end_header_id|>\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:29:57.312245Z","iopub.execute_input":"2025-03-10T03:29:57.312601Z","iopub.status.idle":"2025-03-10T03:29:58.297426Z","shell.execute_reply.started":"2025-03-10T03:29:57.312564Z","shell.execute_reply":"2025-03-10T03:29:58.296359Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model Training","metadata":{}},{"cell_type":"code","source":"# @title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:29:58.298726Z","iopub.execute_input":"2025-03-10T03:29:58.299114Z","iopub.status.idle":"2025-03-10T03:29:58.305621Z","shell.execute_reply.started":"2025-03-10T03:29:58.299077Z","shell.execute_reply":"2025-03-10T03:29:58.304693Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:29:58.306404Z","iopub.execute_input":"2025-03-10T03:29:58.306609Z","iopub.status.idle":"2025-03-10T03:32:27.666886Z","shell.execute_reply.started":"2025-03-10T03:29:58.306590Z","shell.execute_reply":"2025-03-10T03:32:27.665969Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory / max_memory * 100, 3)\nlora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:32:27.667684Z","iopub.execute_input":"2025-03-10T03:32:27.667939Z","iopub.status.idle":"2025-03-10T03:32:27.674728Z","shell.execute_reply.started":"2025-03-10T03:32:27.667916Z","shell.execute_reply":"2025-03-10T03:32:27.673733Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.push_to_hub(\n    commit_message=\"first epoch fine tuning on mixsub\",\n    model_name=TRAINED_MODEL_NAME,\n    # language=\"en\",\n    # finetuned_from=MODEL_NAME,\n    # dataset=DATASET_NAME\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T03:32:27.675875Z","iopub.execute_input":"2025-03-10T03:32:27.676215Z","iopub.status.idle":"2025-03-10T03:32:31.130374Z","shell.execute_reply.started":"2025-03-10T03:32:27.676184Z","shell.execute_reply":"2025-03-10T03:32:31.129621Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model Evaluation","metadata":{}},{"cell_type":"code","source":"# This is to evaluate the fine-tuned model on the eval dataset\n# it will compute the compute metrics for the model\nresults = trainer.evaluate()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(results)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}